import os
import json
from pathlib import Path
from langchain_core.tools import Tool
from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.postprocessor import SentenceTransformerRerank

# å¼•å…¥ Query æ³›åŒ–æ‰€éœ€çš„ LangChain ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å¼•å…¥ OpenAILike
try:
    from llama_index.llms.openai_like import OpenAILike
except ImportError:
    OpenAILike = None

from utils import get_embedding_model
try:
    from llama_index.embeddings.langchain import LangchainEmbedding
except ImportError:
    LangchainEmbedding = None

# --- âœ… æ–°å¢ï¼šQuery æ³›åŒ–å‡½æ•° ---
def stepback_prompting_expansion(query: str, api_key: str = None) -> str:
    """åˆ©ç”¨ LLM å°†å…·ä½“é—®é¢˜æ³›åŒ–ä¸ºé€šç”¨é—®é¢˜"""
    try:
        if not api_key: return query
        
        examples = [
            {"input": "è¿™ç¯‡å…³äºTransformerçš„è®ºæ–‡æ˜¯å¦‚ä½•è§£å†³é•¿æ–‡æœ¬æ•ˆç‡ä½ä¸‹çš„ï¼Ÿ", "output": "LLMå¤„ç†è¶…é•¿æ–‡æœ¬æ—¶çš„ä¸»è¦æŒ‘æˆ˜å’Œæ¶æ„æ”¹è¿›æ–¹æ¡ˆæœ‰å“ªäº›ï¼Ÿ"},
            {"input": "æŒ‡ä»¤å¾®è°ƒå¯¹æå‡æ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰å¸®åŠ©å—ï¼Ÿ", "output": "æŒ‡ä»¤å¾®è°ƒåœ¨æå‡LLMç‰¹å®šä»»åŠ¡èƒ½åŠ›æ–¹é¢æ‰®æ¼”äº†ä»€ä¹ˆè§’è‰²ï¼Ÿ"},
        ]
        example_prompt = ChatPromptTemplate.from_messages([("human", "{input}"), ("ai", "{output}")])
        few_shot_prompt = FewShotChatMessagePromptTemplate(example_prompt=example_prompt, examples=examples)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ–‡çŒ®æ£€ç´¢ä¸“å®¶ï¼Œè¯·å°†ç”¨æˆ·çš„å…·ä½“é—®é¢˜æ³›åŒ–ä¸ºæ›´é€‚åˆæ£€ç´¢çš„é€šç”¨é—®é¢˜ã€‚ä»…è¿”å›æ³›åŒ–åçš„é—®é¢˜ã€‚"),
            few_shot_prompt,
            ("user", "{question}"),
        ])
        
        # ä½¿ç”¨ DashScope è¿›è¡Œæ³›åŒ– (å› ä¸ºç”¨æˆ·ç°åœ¨ä¸»è¦ç”¨è¿™ä¸ª)
        llm = ChatOpenAI(
            model="qwen-plus", 
            temperature=0.1, 
            openai_api_key=api_key, 
            openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        chain = prompt | llm | StrOutputParser()
        expanded_query = chain.invoke({"question": query})
        print(f"ğŸ”„ Query æ³›åŒ–: {query} -> {expanded_query}")
        return expanded_query
    except Exception as e:
        print(f"âš ï¸ æ³›åŒ–å¤±è´¥: {e}")
        return query
# --------------------------------

def get_llamaindex_tool(kb_name, kb_path):
    vs_path = Path(kb_path) / "vectorstore"
    config_path = Path(kb_path) / "config.json"
    
    if not vs_path.exists() or not (vs_path / "docstore.json").exists():
        print(f"çŸ¥è¯†åº“ {kb_name} å°šæœªæ„å»º LlamaIndex ç´¢å¼•ï¼Œè·³è¿‡åŠ è½½ã€‚")
        return None

    try:
        config = {}
        if config_path.exists():
            with open(config_path, "r") as f:
                config = json.load(f)
        
        kb_platform = config.get("platform", "OpenAI")
        kb_embed_model_name = config.get("embedding_model", "text-embedding-3-small")
        rerank_model_name = config.get("rerank_model", "None")

        # 1. é…ç½® Embedding
        embed_model = None
        if LangchainEmbedding:
            lc_embed = get_embedding_model(platform_type=kb_platform, model=kb_embed_model_name)
            embed_model = LangchainEmbedding(lc_embed)

        # 2. é…ç½® LLM (ä½¿ç”¨ OpenAILike ç»•è¿‡éªŒè¯)
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if OpenAILike and api_key:
            llm = OpenAILike(
                model="qwen-plus",
                api_key=api_key,
                api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                is_chat_model=True,
                temperature=0.1,
                timeout=120.0,
                max_retries=2
            )
            Settings.llm = llm

        # 3. åŠ è½½ç´¢å¼•
        storage_context = StorageContext.from_defaults(persist_dir=str(vs_path))
        index = load_index_from_storage(storage_context, embed_model=embed_model)

        # 4. æ··åˆæ£€ç´¢
        vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=6)
        bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=6)
        
        from llama_index.core.retrievers import BaseRetriever
        class HybridRetriever(BaseRetriever):
            def __init__(self, vector, bm25):
                self.vector = vector
                self.bm25 = bm25
                super().__init__()
            def _retrieve(self, query_bundle):
                v_nodes = self.vector.retrieve(query_bundle)
                b_nodes = self.bm25.retrieve(query_bundle)
                all_nodes = {n.node.node_id: n for n in v_nodes}
                for n in b_nodes:
                    if n.node.node_id not in all_nodes:
                        all_nodes[n.node.node_id] = n
                return list(all_nodes.values())

        hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)

        # 5. é‡æ’åº
        node_postprocessors = []
        if rerank_model_name and rerank_model_name != "None":
            try:
                reranker = SentenceTransformerRerank(model=rerank_model_name, top_n=3)
                node_postprocessors.append(reranker)
            except Exception: pass

        # 6. æŸ¥è¯¢å¼•æ“
        query_engine = RetrieverQueryEngine.from_args(
            retriever=hybrid_retriever,
            node_postprocessors=node_postprocessors,
            llm=llm
        )
        
        def query_func(query: str) -> str:
            # --- âœ… æ¢å¤è°ƒç”¨æ³›åŒ–é€»è¾‘ ---
            # åªæœ‰å½“ query æ¯”è¾ƒçŸ­æˆ–è€…æ„å›¾ä¸æ˜ç¡®æ—¶æ‰æ³›åŒ–ï¼Œè¿™é‡Œç®€å•å…¨éƒ¨å°è¯•
            final_query = stepback_prompting_expansion(query, api_key=api_key)
            response = query_engine.query(final_query)
            return str(response)

        return Tool(
            name=f"{kb_name}_knowledge_base_tool",
            func=query_func,
            description=f"ç”¨äºæŸ¥è¯¢å…³äº {kb_name} çš„ä¿¡æ¯ã€‚"
        )
        
    except Exception as e:
        print(f"åŠ è½½çŸ¥è¯†åº“ {kb_name} å¤±è´¥: {e}")
        return None