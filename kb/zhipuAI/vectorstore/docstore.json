{"docstore/metadata": {"/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_0": {"doc_hash": "29c91b6dcbe9e2377a77b6ea9f7a9eb8a3e0a82786bad0afa66cc66925921757"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_1": {"doc_hash": "09037190004edf344dbd54bb602a10777358bac156ea075d3f1a712ae31adc02"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_2": {"doc_hash": "97d48e0a1be13858aa6ef3b3c3476b8f9f923105bc340d68902dcb158299f542"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_3": {"doc_hash": "32c80adcc584594007734271f9bf12ba91ce5fe36d4aa098c5525bd3341d7d12"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_4": {"doc_hash": "736d56bca3e957827a136e5deedf4fd8d925bcc8d78e591cebb772fd7034d76f"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_5": {"doc_hash": "e1a8e14b25407f6f3f0ae89618bcadb444bb2b3aed44529d35c61d482218dbe9"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6": {"doc_hash": "2876e9939447772f562749b03cd3b2c1b112e1f71878d1fcd5c0451aa6b95ae5"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_7": {"doc_hash": "94d66ef3cd76a6f5515cc13e2a1389e81f79e1a297a34589efd7e7eeec840477"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8": {"doc_hash": "172e895711de45a242b432f12fe212a08e6ccae42e321efb37214e00c2aae146"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9": {"doc_hash": "b6a89eea2f3e1d12356eca35e084d1be79b62a7d1602c687c71af8d2543c404d"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10": {"doc_hash": "286111ca07442f3b9471c3907e46c6fae4492ec0e1a8ed8969e31027f30f1bd7"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11": {"doc_hash": "2945e84823e070905a789a16efbdec806e912ba28e5bc8ee48bb99b105fb11e1"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12": {"doc_hash": "2dbfae808b0e7e7c78f2c037ddbc1b2f20d321d3a861b05d8a4037a51be7173e"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13": {"doc_hash": "f1e58467c3943ea5e79017e74021d0843913de48d68d5438efe23f87e18f1446"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_14": {"doc_hash": "dd50bd53902b030714f6246e95f58fed498fee8788a82e2caf5abc0a521f0572"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_15": {"doc_hash": "e884ae0c71c54309852e5464ada87fce9d38d619e4a46582af273eb06cc33464"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_16": {"doc_hash": "b4563731bb9b39dd13914c3a236d4e4a8bcc144d16654d76af666b0e93712481"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_17": {"doc_hash": "76a99ca59c9b3cdc00bc4d20e72e775642fdbd2dec552444b4d3794263ea2466"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_18": {"doc_hash": "f17ac2a7381433e821bdb821dbeb51634bde43e1b2cd14fbf13ee8ad9331d499"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_19": {"doc_hash": "776607cddbab0cea3594f2ac5bfa630eb27e1376c1e09b2983c5b1ec546f17c5"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_20": {"doc_hash": "5006d89cd7bfce8019f578ee813bfa3dc95fa0b187cb7bc730ca45c0abfd493d"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_21": {"doc_hash": "94c1a44a5f9cb77935bc774544c415a028f5cc21697d96971392bb6e05c56152"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_22": {"doc_hash": "4a62fe12b0680f1cef8e266cce1f2d7fdf9c22e2ce2bb6167a507c61da1c5b05"}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_23": {"doc_hash": "b6497261e5a69eae33374beb98dda6bd668827fa0e1bd668ebb5b07cb5100950"}, "b4970a9c-063e-404f-aa74-eadfe34a2616": {"doc_hash": "ea669929eba41f203c454617908cc66606f96e602eb5ec64ecfcf14c401eb19f", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_0"}, "5ed88fe6-954b-4e78-94e4-cfb7dfe0ed3e": {"doc_hash": "484a37ac9a0749acf48d9072c0f30b4790c7fb03853cd2138dc0d42a95bb37a8", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_1"}, "9fc442b1-46aa-40c0-8a3e-0817840c043f": {"doc_hash": "2d91057f0b1842d762aef428e46c07c1037ddae89f5404fdaffc6fd7d9477f86", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_2"}, "5927d359-d09a-4848-b2ce-129a4ae9d59f": {"doc_hash": "fac9acf94148a794670ddac011e381817f0979e6a0a8219474ca8a680bbd4314", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_3"}, "8283eec0-db0d-42f2-b128-0465694de2ab": {"doc_hash": "7ab2681dc0c61f2f0e0349e88d43aeef6ccbd1e438636a8d59d772e3a5068442", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_4"}, "f15f36b0-4edf-4c70-9690-55e1a0c97d2b": {"doc_hash": "bbe374f0b1ade1e99e08cfb4a8fa68f720e8c33372c274173b827059deecda3b", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_5"}, "64f65543-daaa-4a7a-bd8b-cb76bc65a7bd": {"doc_hash": "38af5be423aec20d172772af74f77aca2bc0ba7e1c8b8c425c342e7a4dce2bd7", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6"}, "7bfd65f0-92f2-4752-bba3-787d79780b33": {"doc_hash": "1fde2daf876a46c3f001167d90154ea549eed12d7a57abd1122294352dbb8bb8", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6"}, "0f812708-2448-4a99-ab05-b19bb9805749": {"doc_hash": "c8e54a0012d5dfeeecb3473f88a7a22e4be95360f24803c1b1825dfc82fd6854", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_7"}, "d918648e-dc22-4b61-bc83-b616b14bcde2": {"doc_hash": "baa87c57ddcb65a34a55ce8447c7993a1752da049f87f0e461edf67e052a8b66", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8"}, "cafc907c-ab46-4e32-9af5-3612749cf5ad": {"doc_hash": "0ac6da70140d4de2c3bddb0535469c04949114073c7f68e0922b565140ccd9ec", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8"}, "6e63844d-52d9-4c99-aca8-67f3668f07b7": {"doc_hash": "60b7793612e4ee3e3ce1e6dd656cb6ecb6e226723767519b3da935f7539bf96f", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9"}, "49159fff-e568-4517-875b-4cf66e4514be": {"doc_hash": "69d84afc151c5cd2a16f735b2051c76582cf6614b2275b14b3f1e55e55fe9bac", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9"}, "f361fb5b-4786-409a-aa47-228d4fcd6a0a": {"doc_hash": "820aade2e1a862f45a75dfe955d0bcdda51132b09831dd89813f1c5229a1e3fa", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10"}, "1a091eb7-4ee9-42e4-a95a-6457e71181eb": {"doc_hash": "05f11267669f872e7f4f3e66a9a68cfc785a72789ac0e08d5f65b03d265d7818", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10"}, "40dc2b7e-dec6-4dd0-b737-6e95b5ae49cf": {"doc_hash": "599ebef689cc7eb392236d02e971efc4e26f7a2a9ccd34548ebd790d6c37e8f0", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11"}, "7988e148-f9f8-46cf-acee-9d6e49b397f2": {"doc_hash": "37c8a95f77741dfaffe028816e095acd15d1f5100d7ce7350f63d60a82e9d490", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11"}, "129b4a8b-6e05-4bfc-87a9-65c378b14d89": {"doc_hash": "4859c89bbb41bcb396c9355c027c31977b6ebe44c2c0004aeab1b83301e37b70", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12"}, "78d8d705-5b7c-4e05-86b3-82a7d9276620": {"doc_hash": "809ed86839ab5843bb4203ad46ad2ae743cc6ef73d1bce68cbaa8f9a559d5ed9", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12"}, "89a5e170-b073-44e4-a5e7-fe8540b6854e": {"doc_hash": "8c5025a85dd6aa66ae1034022cd331282cdb522074e3932ec8aab6d2950c4fc4", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13"}, "173d89ec-2055-4dc1-915b-bf2140f37d0e": {"doc_hash": "2f2e448c1775508e186acbb87663c1c7a9a07c03a3a01b183b128102258afa00", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13"}, "c6907c28-d1b9-48c3-ae70-db73b1f646c9": {"doc_hash": "a1c455183af6067ef3051ba30995badd86834d929aed4f43db15b43653a0627a", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_14"}, "4b91da8b-685b-499c-af34-142575b82eda": {"doc_hash": "a71ddc25a5bb87675d7ed2048b75baa8b8e3b74c92d34b184110c95e79f0dc39", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_15"}, "8bf6477e-b700-4502-afe0-8194e976e8e0": {"doc_hash": "69322eb1d8b9a7b5caec35795fef60db18c3e3e8f57fefa4e608bb811fe187e7", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_16"}, "da8ac2e6-58e0-4fd2-bb50-62fd3912e01d": {"doc_hash": "5ed7c68a267c688751227fdcb462ec480f7b72d20ced38e369e8bd14466f418f", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_17"}, "2bbf3636-0872-453e-9a7c-f7b378bcfa16": {"doc_hash": "54dd1f34f1fe3768f3a072c2004e589bf87a9d0c64df405af60b587a737b853d", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_18"}, "1ed1ff13-4a78-4a32-9a82-19f84b6e97de": {"doc_hash": "a27f2d626dec3b158644858028922db5e54303d5699c6a76f2b7645e757e72f3", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_19"}, "f4ea8c22-1a9f-46dd-ae09-30d0c94fb6a4": {"doc_hash": "8bd1bfd4980051e0e411893a7132c8338924e879d29135d9460d6ce8262c1c69", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_20"}, "d1ef78bf-7dad-4493-b3c0-c7ce9591f813": {"doc_hash": "89df13cbf51989563c2831e1432f9ab9d0283b88b5b399dbd316a9fbd0b00f24", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_21"}, "704d6582-6784-424c-9bf7-135ca5ec149f": {"doc_hash": "e475ff8d7dd902e4be157281d551f91f47ae26642bfbcece978981645fe73f45", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_22"}, "73f1b6a3-cd38-4aac-bbf4-9cd14d6f437c": {"doc_hash": "c7bd09f8cda90196ca28ed117086f3da0b7c59a67744ebc3384420543385881f", "ref_doc_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_23"}}, "docstore/ref_doc_info": {"/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_0": {"node_ids": ["b4970a9c-063e-404f-aa74-eadfe34a2616"], "metadata": {"page_label": "1", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_1": {"node_ids": ["5ed88fe6-954b-4e78-94e4-cfb7dfe0ed3e"], "metadata": {"page_label": "2", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_2": {"node_ids": ["9fc442b1-46aa-40c0-8a3e-0817840c043f"], "metadata": {"page_label": "3", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_3": {"node_ids": ["5927d359-d09a-4848-b2ce-129a4ae9d59f"], "metadata": {"page_label": "4", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_4": {"node_ids": ["8283eec0-db0d-42f2-b128-0465694de2ab"], "metadata": {"page_label": "5", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_5": {"node_ids": ["f15f36b0-4edf-4c70-9690-55e1a0c97d2b"], "metadata": {"page_label": "6", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6": {"node_ids": ["64f65543-daaa-4a7a-bd8b-cb76bc65a7bd", "7bfd65f0-92f2-4752-bba3-787d79780b33"], "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_7": {"node_ids": ["0f812708-2448-4a99-ab05-b19bb9805749"], "metadata": {"page_label": "8", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8": {"node_ids": ["d918648e-dc22-4b61-bc83-b616b14bcde2", "cafc907c-ab46-4e32-9af5-3612749cf5ad"], "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9": {"node_ids": ["6e63844d-52d9-4c99-aca8-67f3668f07b7", "49159fff-e568-4517-875b-4cf66e4514be"], "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10": {"node_ids": ["f361fb5b-4786-409a-aa47-228d4fcd6a0a", "1a091eb7-4ee9-42e4-a95a-6457e71181eb"], "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11": {"node_ids": ["40dc2b7e-dec6-4dd0-b737-6e95b5ae49cf", "7988e148-f9f8-46cf-acee-9d6e49b397f2"], "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12": {"node_ids": ["129b4a8b-6e05-4bfc-87a9-65c378b14d89", "78d8d705-5b7c-4e05-86b3-82a7d9276620"], "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13": {"node_ids": ["89a5e170-b073-44e4-a5e7-fe8540b6854e", "173d89ec-2055-4dc1-915b-bf2140f37d0e"], "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_14": {"node_ids": ["c6907c28-d1b9-48c3-ae70-db73b1f646c9"], "metadata": {"page_label": "15", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_15": {"node_ids": ["4b91da8b-685b-499c-af34-142575b82eda"], "metadata": {"page_label": "16", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_16": {"node_ids": ["8bf6477e-b700-4502-afe0-8194e976e8e0"], "metadata": {"page_label": "17", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_17": {"node_ids": ["da8ac2e6-58e0-4fd2-bb50-62fd3912e01d"], "metadata": {"page_label": "18", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_18": {"node_ids": ["2bbf3636-0872-453e-9a7c-f7b378bcfa16"], "metadata": {"page_label": "19", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_19": {"node_ids": ["1ed1ff13-4a78-4a32-9a82-19f84b6e97de"], "metadata": {"page_label": "20", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_20": {"node_ids": ["f4ea8c22-1a9f-46dd-ae09-30d0c94fb6a4"], "metadata": {"page_label": "21", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_21": {"node_ids": ["d1ef78bf-7dad-4493-b3c0-c7ce9591f813"], "metadata": {"page_label": "22", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_22": {"node_ids": ["704d6582-6784-424c-9bf7-135ca5ec149f"], "metadata": {"page_label": "23", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}, "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_23": {"node_ids": ["73f1b6a3-cd38-4aac-bbf4-9cd14d6f437c"], "metadata": {"page_label": "24", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}}}, "docstore/data": {"b4970a9c-063e-404f-aa74-eadfe34a2616": {"__data__": {"id_": "b4970a9c-063e-404f-aa74-eadfe34a2616", "embedding": null, "metadata": {"page_label": "1", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "29c91b6dcbe9e2377a77b6ea9f7a9eb8a3e0a82786bad0afa66cc66925921757", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Qwen-VL: A Versatile Vision-Language Model for\nUnderstanding, Localization, Text Reading, and Beyond\nJinze Bai\u2217 Shuai Bai\u2217 Shusheng Yang\u2217 Shijie Wang Sinan Tan\nPeng Wang Junyang Lin Chang Zhou \u2020 Jingren Zhou\nAlibaba Group\nCode & Demo & Models:https://github.com/QwenLM/Qwen-VL\nAbstract\nInthiswork,weintroducetheQwen-VLseries,asetoflarge-scalevision-languagemodels\n(LVLMs) designed to perceive and understand both texts and images. Starting from the\nQwen-LM as a foundation, we endow it with visual capacity by the meticulously de-\nsigned(i) visual receptor,(ii) input-output interface,(iii) 3-stage training pipeline, and\n(iv) multilingual multimodal cleaned corpus. Beyond the conventional image descrip-\ntion and question-answering, we implement the grounding and text-reading ability of\nQwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-\nVL and Qwen-VL-Chat, set new records for generalist models under similar model scales\non a broad range of visual-centric benchmarks (e.g., image captioning, question answer-\ning, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on\nreal-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates\nsuperiority compared to existing vision-language chatbots. All models are public to\nfacilitate future research.\nFigure 1: Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other\ngeneralist models.\n\u2217Equal contribution,\u2020Corresponding author\n1\narXiv:2308.12966v3  [cs.CV]  13 Oct 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ed88fe6-954b-4e78-94e4-cfb7dfe0ed3e": {"__data__": {"id_": "5ed88fe6-954b-4e78-94e4-cfb7dfe0ed3e", "embedding": null, "metadata": {"page_label": "2", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "09037190004edf344dbd54bb602a10777358bac156ea075d3f1a712ae31adc02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2: Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple\nimage inputs, multi-round dialogue, multilingual conversation, text-reading, localization, fine-grained\nrecognition and understanding ability.\n1 Introduction\nRecently, Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Gao et al.,\n2023; Qwen, 2023) have attracted wide attention due to their powerful capabilities in text generation and\ncomprehension. These models can be further aligned with user intent through fine-tuning instructions,\nshowcasing strong interactive capabilities and the potential to enhance productivity as intelligent assistants.\nHowever, native large language models only live in the pure-text world, lacking the ability to handle other\ncommon modalities (such as images, speech, and videos), resulting in great restrictions on their application\nscope. Motivated by this, a group of Large Vision Language Models (LVLMs) (Alayrac et al., 2022; Chen\net al., 2022; Li et al., 2023c; Dai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al.,\n2023; Ye et al., 2023b,a; Chen et al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023; OpenAI, 2023)\nhave been developed to enhance large language models with the ability to perceive and understand visual\nsignals. These large-scale vision-language models demonstrate promising potential in solving real-world\nvision-central problems.\nNevertheless, despite that lots of works have been conducted to explore the limitation and potency of LVLMs,\ncurrent open-source LVLMs always suffer from inadequate training and optimization, thus lag far behind\nthe proprietary models (Chen et al., 2022, 2023b; OpenAI, 2023), which hinders further exploration and\napplication of LVLMs in open-source community. What\u2019s more, as real-world visual scenarios are quite\ncomplicated, fine-grained visual understanding plays a crucial role for LVLMs to assist people effectively\nand precisely. But only a few attempts had been made toward this direction (Peng et al., 2023; Chen et al.,\n2023a), the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and\nlacking the ability to execute fine-grained perception such as object grounding or text reading.\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fc442b1-46aa-40c0-8a3e-0817840c043f": {"__data__": {"id_": "9fc442b1-46aa-40c0-8a3e-0817840c043f", "embedding": null, "metadata": {"page_label": "3", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_2", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "97d48e0a1be13858aa6ef3b3c3476b8f9f923105bc340d68902dcb158299f542", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper, we explore a way out and present the newest members of the open-sourced Qwen families:\nQwen-VL series. Qwen-VLs are a series of highly performant and versatile vision-language foundation\nmodels based on Qwen-7B (Qwen, 2023) language model. We empower the LLM basement with visual\ncapacity by introducing a new visual receptor including a language-aligned visual encoder and a position-\naware adapter. The overall model architecture as well as the input-output interface are quite concise and\nwe elaboratedly design a 3-stage training pipeline to optimize the whole model upon a vast collection of\nimage-text corpus.\nOur pre-trained checkpoint, termed Qwen-VL, is capable of perceiving and understanding visual inputs,\ngenerating desired responses according to given prompts, and accomplishing various vision-language tasks\nsuch as image captioning, question answering, text-oriented question answering, and visual grounding.\nQwen-VL-Chat is the instruction-tuned vision-language chatbot based on Qwen-VL. As shown in Fig. 2,\nQwen-VL-Chat is able to interact with users and perceive the input images following the intention of users.\nSpecifically, the features of the Qwen-VL series models include:\n\u2022 Leading performance: Qwen-VLs achieve top-tier accuracy on a vast of vision-centric understanding\nbenchmarks compared to counterparts with similar scales. Besides, Qwen-VL\u2019s stuning performance\ncovers not only the conventional benchmarkse.g., captioning, question-answering, grounding), but\nalso some recently introduced dialogue benchmarks.\n\u2022 Multi-lingual: Similar to Qwen-LM, Qwen-VLs are trained upon multilingual image-text data with a\nconsiderable amount of corpus being in English and Chinese. In this way, Qwen-VLs naturally support\nEnglish, Chinese, and multilingual instructions.\n\u2022 Multi-image: In the training phase, we allow arbitrary interleaved image-text data as Qwen-VL\u2019s inputs.\nThis feature allows our Qwen-Chat-VL to compare, understand, and analyze the context when multiple\nimages are given.\n\u2022 Fine-grained visual understanding: Thanks to the higher-resolution input size and fine-grained corpus\nwe used in training, Qwen-VLs exhibit highly competitive fine-grained visual understanding ability.\nCompared to existing vision-language generalists, our Qwen-VLs possess much better grounding,\ntext-reading, text-oriented question answering, and fine-grained dialog performance.\n2 Methodology\n2.1 Model Architecture\nTheoverallnetworkarchitectureofQwen-VLconsistsofthreecomponentsandthedetailsofmodelparameters\nare shown in Table 1:\nLarge Language Model: Qwen-VL adopts a large language model as its foundation component. The model\nis initialized with pre-trained weights from Qwen-7B (Qwen, 2023).\nVisual Encoder: The visual encoder of Qwen-VL uses the Vision Transformer (ViT) (Dosovitskiy et al., 2021)\narchitecture, initialized with pre-trained weights from Openclip\u2019s ViT-bigG (Ilharco et al., 2021). During\nboth training and inference, input images are resized to a specific resolution. The visual encoder processes\nimages by splitting them into patches with a stride of 14, generating a set of image features.\nPosition-aware Vision-Language Adapter: To alleviate the efficiency issues arising from long image feature\nsequences, Qwen-VL introduces a vision-language adapter that compresses the image features. This adapter\ncomprises a single-layer cross-attention module initialized randomly. The module uses a group of trainable\nvectors (Embeddings) as query vectors and the image features from the visual encoder as keys for cross-\nattention operations. This mechanism compresses the visual feature sequence to a fixed length of 256. The\nablation about the number of queries is shown in Appendix E.2. Additionally, considering the significance\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5927d359-d09a-4848-b2ce-129a4ae9d59f": {"__data__": {"id_": "5927d359-d09a-4848-b2ce-129a4ae9d59f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "32c80adcc584594007734271f9bf12ba91ce5fe36d4aa098c5525bd3341d7d12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of positional information for fine-grained image comprehension, 2D absolute positional encodings are\nincorporated into the cross-attention mechanism\u2019s query-key pairs to mitigate the potential loss of positional\ndetails during compression. The compressed image feature sequence of length 256 is subsequently fed into\nthe large language model.\nTable 1: Details of Qwen-VL model parameters.\nVision Encoder VL Adapter LLM Total\n1.9B 0.08B 7.7B 9.6B\nQwenLM\nViT \nStage1: Pretraining\nImage-Text Pairs\nQwenLM\nStage2:Multi-task \nPretraining\nMulti-task and \nInterleaved VL Data\nStage3: Supervised \nFinetuning\nViT \nQwenLM\nViT \nChat Interleaved \nVL Data\nLow Resolution\n High Resolution\n High Resolution\nCrossAttn\nLearnable\nQuery\nEmbs\nCrossAttn\nLearnable\nQuery\nEmbs\nCrossAttn\nLearnable\nQuery\nEmbs\nFigure 3: The training pipeline of the Qwen-VL series.\n2.2 Inputs and Outputs\nImage Input: Images are processed through the visual encoder and adapter, yielding fixed-length sequences\nof image features. To differentiate between image feature input and text feature input, two special tokens\n(<img> and</img>) are appended to the beginning and end of the image feature sequence respectively,\nsignifying the start and end of image content.\nBounding Box Input and Output: Toenhancethemodel\u2019scapacityforfine-grainedvisualunderstandingand\ngrounding, Qwen-VL\u2019s training involves data in the form of region descriptions, questions, and detections.\nDiffering from conventional tasks involving image-text descriptions or questions, this task necessitates the\nmodel\u2019s accurate understanding and generation of region descriptions in a designated format. For any\ngiven bounding box, a normalization process is applied (within the range [0, 1000)) and transformed into a\nspecified string format: \"(Xtopleft , Ytopleft ), (Xbottomright, Ybottomright)\". The string is tokenized as text and\ndoes not require an additional positional vocabulary. To distinguish between detection strings and regular\ntext strings, two special tokens (<box> and</box> are added at the beginning and end of the bounding\nbox string. Additionally, to appropriately associate bounding boxes with their corresponding descriptive\nwords or sentences, another set of special tokens (<ref> and</ref>) is introduced, marking the content\nreferred to by the bounding box.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8283eec0-db0d-42f2-b128-0465694de2ab": {"__data__": {"id_": "8283eec0-db0d-42f2-b128-0465694de2ab", "embedding": null, "metadata": {"page_label": "5", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "736d56bca3e957827a136e5deedf4fd8d925bcc8d78e591cebb772fd7034d76f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 Training\nAs illustrated in Fig. 3, the training process of the Qwen-VL model consists of three stages: two stages of\npre-training and a final stage of instruction fine-tuning training.\n3.1 Pre-training\nInthefirststageofpre-training,wemainlyutilizealarge-scale,weaklylabeled,web-crawledsetofimage-text\npairs. Our pre-training dataset is composed of several publicly accessible sources and some in-house data.\nWe made an effort to clean the dataset of certain patterns. As summarized in Table 2, the original dataset\ncontains a total of 5 billion image-text pairs, and after cleaning, 1.4 billion data remain, with 77.3% English\n(text) data and 22.7% Chinese (text) data.\nTable 2: Details of Qwen-VL pre-training data. LAION-en and LAION-zh are the English and Chinese\nlanguage subset of LAION-5B (Schuhmann et al., 2022a). LAION-COCO (Schuhmann et al., 2022b) is a\nsyntheticdatasetgeneratedfromLAION-en. DataComp(Gadreetal.,2023)andCoyo(Byeonetal.,2022)are\ncollectionsofimage-textpairs. CC12M(Changpinyoetal.,2021),CC3M(Sharmaetal.,2018),SBU(Ordonez\net al., 2011) and COCO Caption (Chen et al., 2015) are academic caption datasets.\nLanguage Dataset Original Cleaned Remaining%\nEnglish\nLAION-en 2B 280M 14%\nLAION-COCO 600M 300M 50%\nDataComp 1.4B 300M 21%\nCoyo 700M 200M 28%\nCC12M 12M 8M 66%\nCC3M 3M 3M 100%\nSBU 1M 0.8M 80%\nCOCO Caption 0.6M 0.6M 100%\nChinese LAION-zh 108M 105M 97%\nIn-house Data 220M 220M 100%\nTotal 5B 1.4B 28%\nWe freeze the large language model and only optimize the vision encoder and VL adapter in this stage.\nThe input images are resized to224 \u00d7 224. The training objective is to minimize the cross-entropy of the\ntext tokens. The maximum learning rate is2e\u22124 and the training process uses a batch size of 30720 for the\nimage-text pairs, and the entire first stage of pre-training lasts for 50,000 steps, consuming approximately 1.5\nbillion image-text samples. More hyperparameters are detailed in Appendix C and the convergence curve of\nthis stage is shown in Figure 6.\n3.2 Multi-task Pre-training\nIn the second stage of multi-task pre-training, we introduce high-quality and fine-grained VL annotation\ndata with a larger input resolution and interleaved image-text data. As summarized in Table 3, we trained\nQwen-VL on 7 tasks simultaneously. For text generation, we use the in-house collected corpus to maintain\nthe LLM\u2019s ability. Captioning data is the same with Table 2 except for far fewer samples and excluding\nLAION-COCO. We use a mixture of publicly available data for the VQA task which includes GQA (Hudson\nandManning,2019),VGQA(Krishnaetal.,2017),VQAv2(Goyaletal.,2017),DVQA(Kafleetal.,2018),OCR-\nVQA (Mishra et al., 2019) and DocVQA (Mathew et al., 2021). We follow Kosmos-2 to use the GRIT (Peng\net al., 2023) dataset for the grounding task with minor modifications. For the reference grounding and\ngrounded captioning duality tasks, we construct training samples from GRIT (Peng et al., 2023), Visual\nGenome (Krishna et al., 2017), RefCOCO (Kazemzadeh et al., 2014), RefCOCO+, and RefCOCOg (Mao et al.,\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f15f36b0-4edf-4c70-9690-55e1a0c97d2b": {"__data__": {"id_": "f15f36b0-4edf-4c70-9690-55e1a0c97d2b", "embedding": null, "metadata": {"page_label": "6", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "e1a8e14b25407f6f3f0ae89618bcadb444bb2b3aed44529d35c61d482218dbe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2016). In order to improve the text-oriented tasks, we collect pdf and HTML format data from Common\nCrawl1 and generate synthetic OCR data in English and Chinese language with natural scenery background,\nfollowing (Kim et al., 2022). Finally, we simply construct interleaved image-text data by packing the same\ntask data into sequences of length 2048.\nTable 3: Details of Qwen-VL multi-task pre-training data.\nTask # Samples Dataset\nCaptioning 19.7M LAION-en & zh, DataComp, Coyo, CC12M & 3M, SBU,\nCOCO, In-house Data\nVQA 3.6M GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA,\nTextVQA, ChartQA, AI2D\nGrounding2 3.5M GRIT\nRef Grounding 8.7M GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg\nGrounded Cap. 8.7M GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg\nOCR 24.8M SynthDoG-en & zh, Common Crawl pdf & HTML\nPure-text Autoregression 7.8M In-house Data\nWe increase the input resolution of the visual encoder from224 \u00d7 224 to448 \u00d7 448, reducing the information\nloss caused by image down-sampling. Besides, we ablate the window attention and global attention for\nhigher resolutions of the vision transformer in Appendix E.3. We unlocked the large language model and\ntrained the whole model. The training objective is the same as the pre-training stage.\n3.3 Supervised Fine-tuning\nDuring this stage, we finetuned the Qwen-VL pre-trained model through instruction fine-tuning to enhance\nits instruction following and dialogue capabilities, resulting in the interactive Qwen-VL-Chat model. The\nmulti-modal instruction tuning data primarily comes from caption data or dialogue data generated through\nLLM self-instruction, which often only addresses single-image dialogue and reasoning and is limited to\nimage content comprehension. We construct an additional set of dialogue data through manual annotation,\nmodel generation, and strategy concatenation to incorporate localization and multi-image comprehension\nabilities into the Qwen-VL model. We confirm that the model effectively transfers these capabilities to a\nwider range of languages and question types. Additionally, we mix multi-modal and pure text dialogue\ndata during training to ensure the model\u2019s universality in dialogue capabilities. The instruction tuning data\namounts to 350k. In this stage, we freeze the visual encoder and optimize the language model and adapter\nmodule. We demonstrate the data format of this stage in Appendix B.2.\n4 Evaluation\nIn this section, we conduct an overall evaluation on various multi-modal tasks to comprehensively assess\nour models\u2019 visual understanding ability. In the following, Qwen-VL denotes the model after the multi-task\ntraining, and Qwen-VL-Chat denotes the model after supervised fine-tuning (SFT) stage.\nTable 9 provides a detailed summary of the used evaluation benchmarks and corresponding metrics.\n4.1 Image Caption and General Visual Question Answering\nImage caption and general visual question answering (VQA) are two conventional tasks for vision-language\nmodels. Specifically,imagecaptionrequiresthemodeltogenerateadescriptionforagivenimageandgeneral\nVQA requires the model to generate an answer for a given image-question pair.\n1https://digitalcorpora.org/corpora/file-corpora/cc-main-2021-31-pdf-untruncated\n2This task is to generate noun/phrase grounded captions (Peng et al., 2023).\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64f65543-daaa-4a7a-bd8b-cb76bc65a7bd": {"__data__": {"id_": "64f65543-daaa-4a7a-bd8b-cb76bc65a7bd", "embedding": null, "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2876e9939447772f562749b03cd3b2c1b112e1f71878d1fcd5c0451aa6b95ae5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bfd65f0-92f2-4752-bba3-787d79780b33", "node_type": "1", "metadata": {}, "hash": "ef097f160bed9fe61532694f224d10bc349f02c34eee418fe511b323bc11bc98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4: Results on Image Captioning and General VQA.\nModel Type Model Image Caption General VQA\nNocaps\n(0-shot)\nFlickr30K\n(0-shot) VQAv2 OKVQA GQA SciQA-Img\n(0-shot)\nVizWiz\n(0-shot)\nGeneralist\nModels\nFlamingo-9B - 61.5 51.8 44.7 - - 28.8\nFlamingo-80B - 67.2 56.3 50.6 - - 31.6\nUnified-IO-XL 100.0 - 77.9 54.0 - - -\nKosmos-1 - 67.1 51.0 - - - 29.2\nKosmos-2 - 80.5 51.1 - - - -\nBLIP-2 (Vicuna-13B) 103.9 71.6 65.0 45.9 32.3 61.0 19.6\nInstructBLIP (Vicuna-13B) 121.9 82.8 - - 49.5 63.1 33.4\nShikra (Vicuna-13B) - 73.9 77.36 47.16 - - -\nQwen-VL (Qwen-7B) 121.4 85.8 79.5 58.6 59.3 67.1 35.2\nQwen-VL-Chat 120.2 81.0 78.2 56.6 57.5 68.2 38.9\nSpecialist\nSOTAs - 127.0\n(PALI-17B)\n84.5\n(InstructBLIP\n-FlanT5-XL)\n86.1\n(PALI-X\n-55B)\n66.1\n(PALI-X\n-55B)\n72.1\n(CFR)\n92.53\n(LLaVa+\nGPT-4)\n70.9\n(PALI-X\n-55B)\nFor the image caption task, we choose Nocaps (Agrawal et al., 2019) and Flickr30K (Young et al., 2014) as\nbenchmarks and report CIDEr score (Vedantam et al., 2015) as metric. We utilize greedy search for caption\ngeneration with a prompt of\"Descripe the image in English:\".\nFor general VQA, we utilize five benchmarks including VQAv2 (Goyal et al., 2017), OKVQA (Marino et al.,\n2019),GQA(HudsonandManning,2019),ScienceQA(ImageSet)(Luetal.,2022b)andVizWizVQA(Gurari\net al., 2018). For VQAv2, OKVQA, GQA and VizWiz VQA, we employ open-ended answer generation with\ngreedy decoding strategy and a prompt of\"{question} Answer:\", without any constrain on model\u2019s output\nspace. However, forScienceQA,weconstrainthemodel\u2019soutputtopossibleoptions(insteadofopen-ended),\nchoose the option with highest confidence as model\u2019s prediction, and report the Top-1 accuracy.\nThe overall performance on image caption and general VQA tasks are reported in Table 4. As the results\nshown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous\ngeneralist models in terms of both two tasks. Specifically, on zero-shot image caption task, Qwen-VL achieves\nstate-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms\nprevious generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).\nOn general VQA benchmarks, our models also exhibit distinct advantages compared to others. On VQAv2,\nOKVQAandGQAbenchmarks, Qwen-VLachieves79.5, 58.6and59.3accuracyrespectively, whichsurpasses\nrecent proposed LVLMs by a large margin. It\u2019s worth noting that Qwen-VL also shows strong zero-shot\nperformance on ScienceQA and VizWiz datasets.\n4.2 Text-oriented Visual Question Answering\nText-oriented visual understanding has a broad application prospect in real-world scenarios.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bfd65f0-92f2-4752-bba3-787d79780b33": {"__data__": {"id_": "7bfd65f0-92f2-4752-bba3-787d79780b33", "embedding": null, "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2876e9939447772f562749b03cd3b2c1b112e1f71878d1fcd5c0451aa6b95ae5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64f65543-daaa-4a7a-bd8b-cb76bc65a7bd", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "38af5be423aec20d172772af74f77aca2bc0ba7e1c8b8c425c342e7a4dce2bd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, on zero-shot image caption task, Qwen-VL achieves\nstate-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms\nprevious generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).\nOn general VQA benchmarks, our models also exhibit distinct advantages compared to others. On VQAv2,\nOKVQAandGQAbenchmarks, Qwen-VLachieves79.5, 58.6and59.3accuracyrespectively, whichsurpasses\nrecent proposed LVLMs by a large margin. It\u2019s worth noting that Qwen-VL also shows strong zero-shot\nperformance on ScienceQA and VizWiz datasets.\n4.2 Text-oriented Visual Question Answering\nText-oriented visual understanding has a broad application prospect in real-world scenarios. We assess our\nmodels\u2019abilitytowardtext-orientedvisualquestionansweringonseveralbenchmarksincludingTextVQA(Sidorov\net al., 2020), DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022), AI2Diagram (Kembhavi et al.,\n2016), and OCR-VQA (Mishra et al., 2019). Similarly, the results are shown in Table 5. Compared to previous\ngeneralist models and recent LVLMs, our models show better performance on most benchmarks, frequently\nby a large margin.\n4.3 Refer Expression Comprehension\nWe show our models\u2019 fine-grained image understanding and localization ability by evaluating on a sort of\nrefer expression comprehension benchmarks such as RefCOCO (Kazemzadeh et al., 2014), RefCOCOg (Mao\net al., 2016), RefCOCO+ (Mao et al., 2016) and GRIT (Gupta et al., 2022). Specifically, the refer expression\ncomprehension task requires the model to localize the target object under the guidance of a description. The\n7", "mimetype": "text/plain", "start_char_idx": 1892, "end_char_idx": 3541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f812708-2448-4a99-ab05-b19bb9805749": {"__data__": {"id_": "0f812708-2448-4a99-ab05-b19bb9805749", "embedding": null, "metadata": {"page_label": "8", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "94d66ef3cd76a6f5515cc13e2a1389e81f79e1a297a34589efd7e7eeec840477", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 5: Results on Text-oriented VQA.\nModel type Model TextVQA DocVQA ChartQA AI2D OCR-VQA\nGeneralist Models\nBLIP-2 (Vicuna-13B) 42.4 - - - -\nInstructBLIP (Vicuna-13B) 50.7 - - - -\nmPLUG-DocOwl (LLaMA-7B) 52.6 62.2 57.4 - -\nPix2Struct-Large (1.3B) - 76.6 58.6 42.1 71.3\nQwen-VL (Qwen-7B) 63.8 65.1 65.7 62.3 75.7\nQwen-VL-Chat 61.5 62.6 66.3 57.7 70.5\nSpecialist SOTAs PALI-X-55B (Single-task fine-\ntuning, without OCR Pipeline) 71.44 80.0 70.0 81.2 75.0\nTable 6: Results on Referring Expression Comprehension task.\nModel type Model RefCOCO RefCOCO+ RefCOCOg GRIT\nval test-A test-B val test-A test-B val test refexp\nGeneralist Models\nGPV-2 - - - - - - - - 51.50\nOFA-L* 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58 61.70\nUnified-IO - - - - - - - - 78.61\nVisionLLM-H 86.70 - - - - - - -\nShikra-7B 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 69.34\nShikra-13B 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16 69.03\nQwen-VL-7B 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48 78.22\nQwen-VL-7B-Chat 88.55 92.27 84.51 82.82 88.59 76.79 85.96 86.32 -\nSpecialist SOTAs\nG-DINO-L 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02 -\nUNINEXT-H 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37 -\nONE-PEACE 92.58 94.18 89.26 88.77 92.21 83.23 89.22 89.27 -\nresults are shown in Table 6. Compared to previous generalist models or recent LVLMs, our models obtain\ntop-tier results on all benchmarks.\n4.4 Few-shot Learning on Vision-Language Tasks\nOur model also exhibits satisfactory in-context learning (a.k.a., few-shot learning) ability. As shown in\nFigure 4, Qwen-VL achieves better performance through in-context few-shot learning on OKVQA (Marino\net al., 2019), Vizwiz (Gurari et al., 2018), TextVQA (Sidorov et al., 2020), and Flickr30k (Young et al.,\n2014) when compared with models with similar number of parameters (Flamingo-9B(Alayrac et al., 2022),\nOpenFlamingo-9B(?) and IDEFICS-9B?). Qwen-VL\u2019s performance is even comparable with much larger\nmodels(Flamingo-80BandIDEFICS-80B).Notethatweadoptna\u00efverandomsampletoconstructthefew-shot\nexemplars, sophisticated few-shot exemplar construction methods such as RICES (Yang et al., 2022b) are not\nused despite better results would be achieved.\nFigure 4: Few-shot learning results of Qwen-VL in comparison with other models.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d918648e-dc22-4b61-bc83-b616b14bcde2": {"__data__": {"id_": "d918648e-dc22-4b61-bc83-b616b14bcde2", "embedding": null, "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "172e895711de45a242b432f12fe212a08e6ccae42e321efb37214e00c2aae146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cafc907c-ab46-4e32-9af5-3612749cf5ad", "node_type": "1", "metadata": {}, "hash": "2745b7f453b976fd1f6abcbac2643f93fbe6f885115e8cf614935ec561e32b04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 7: Results on Instruction-following benchmarks.\nModel TouchStone SEED-Bench MME\nEn Cn All Img Video Perception Cognition\nVisualGLM - 247.1 - - - 705.31 181.79\nPandaGPT 488.5 - - - - 642.59 228.57\nMiniGPT4 531.7 - 42.8 47.4 29.9 581.67 144.29\nInstructBLIP 552.4 - 53.4 58.8 38.1 1212.82 291.79\nLLaMA-AdapterV2 590.1 - 32.7 35.2 25.8 972.67 248.93\nLLaVA 602.7 - 33.5 37.0 23.8 502.82 214.64\nmPLUG-Owl 605.4 - 34.0 37.9 23.0 967.34 276.07\nQwen-VL - - 56.3 62.3 39.1 - -\nQwen-VL-Chat 645.2 401.2 58.2 65.4 37.8 1487.58 360.71\n4.5 Instruction Following in Real-world User Behavior\nIn addition to previous conventional vision-language evaluations, to evaluate our Qwen-VL-Chat model\u2019s\ncapacity under real-world user behavior, we further conduct the evaluations on the TouchStone (Bai et al.,\n2023), SEED-Bench (Li et al., 2023b), and MME (Fu et al., 2023). TouchStone is an open-ended vision-\nlanguage instruction-following benchmark. We compare the instruction-following ability of Qwen-VL-Chat\nwithotherinstruction-tunedLVLMsinbothEnglishandChineseontheTouchStonebenchmark. SEED-Bench\nconsistsof19Kmultiple-choicequestionswithaccuratehumanannotationsforevaluatingMultimodalLLMs,\ncovering 12 evaluation dimensions including both the spatial and temporal understanding. MME measures\nboth perception and cognition abilities on a total of 14 subtasks.\nThe results on three benchmarks are shown in Table 7. Qwen-VL-Chat has achieved obvious advantages\nover other LVLMs on all three datasets, indicating that our model performs better in understanding and\nanswering diverse user instructions. In SEED-Bench, we have found that our model\u2019s visual capabilities\ncan be effectively transferred to video tasks by simply sampling four frames. In terms of the overall scores\npresented in TouchStone, our model demonstrates a clear advantage compared to other LVLMs, especially\nin terms of its Chinese capabilities. In terms of the broad categories of abilities, our model exhibits a more\npronounced advantage in understanding and recognition, particularly in areas such as text recognition and\nchart analysis. For more detailed information, please refer to the TouchStone dataset.\n5 Related Work\nIn recent years, researchers have shown considerable interest in vision-language learning (Su et al., 2019;\nChen et al., 2020; Li et al., 2020; Zhang et al., 2021; Li et al., 2021b; Lin et al., 2021; Kim et al., 2021; Dou\net al., 2022; Zeng et al., 2021; Li et al., 2021a, 2022), especially in the development of multi-task generalist\nmodels (Hu and Singh, 2021; Singh et al., 2022; Zhu et al., 2022; Yu et al., 2022; Wang et al., 2022a; Lu et al.,\n2022a; Bai et al., 2022). CoCa (Yu et al., 2022) proposes an encoder-decoder structure to address image-text\nretrieval and vision-language generation tasks simultaneously. OFA (Wang et al., 2022a) transforms specific\nvision-language tasks into sequence-to-sequence tasks using customized task instructions. Unified I/O (Lu\net al., 2022a) further introduces more tasks like segmentation and depth estimation into a unified framework.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cafc907c-ab46-4e32-9af5-3612749cf5ad": {"__data__": {"id_": "cafc907c-ab46-4e32-9af5-3612749cf5ad", "embedding": null, "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "172e895711de45a242b432f12fe212a08e6ccae42e321efb37214e00c2aae146", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d918648e-dc22-4b61-bc83-b616b14bcde2", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "baa87c57ddcb65a34a55ce8447c7993a1752da049f87f0e461edf67e052a8b66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CoCa (Yu et al., 2022) proposes an encoder-decoder structure to address image-text\nretrieval and vision-language generation tasks simultaneously. OFA (Wang et al., 2022a) transforms specific\nvision-language tasks into sequence-to-sequence tasks using customized task instructions. Unified I/O (Lu\net al., 2022a) further introduces more tasks like segmentation and depth estimation into a unified framework.\nAnother category of research focuses on building vision-language representation models (Radford et al.,\n2021; Jia et al., 2021; Zhai et al., 2022; Yuan et al., 2021; Yang et al., 2022a). CLIP (Radford et al., 2021)\nleverages contrastive learning and large amounts of data to align images and language in a semantic space,\nresulting in strong generalization capabilities across a wide range of downstream tasks. BEIT-3 (Wang\net al., 2022b) employs a mixture-of-experts (MOE) structure and unified masked token prediction objective,\nachieving state-of-the-art results on various visual-language tasks. In addition to vision-language learning,\nImageBind (Girdhar et al., 2023) and ONE-PEACE (Wang et al., 2023) align more modalities such as speech\ninto a unified semantic space, thus creating more general representation models.\nDespite achieving significant progress, previous vision-language models still have several limitations such\n9", "mimetype": "text/plain", "start_char_idx": 2660, "end_char_idx": 4002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e63844d-52d9-4c99-aca8-67f3668f07b7": {"__data__": {"id_": "6e63844d-52d9-4c99-aca8-67f3668f07b7", "embedding": null, "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "b6a89eea2f3e1d12356eca35e084d1be79b62a7d1602c687c71af8d2543c404d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49159fff-e568-4517-875b-4cf66e4514be", "node_type": "1", "metadata": {}, "hash": "5c2ed9dc5ff0af6b5ed5454076b0c89158b1d495b0f8ce5810769a08b6b3e7bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack\nof in-context abilities. With the rapid development of large language models (LLMs) (Brown et al., 2020;\nOpenAI,2023;Aniletal.,2023;Gaoetal.,2023;Qwen,2023),researchershavestartedbuildingmorepowerful\nlarge vision-language models (LVLMs) based on LLMs (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2023c;\nDai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al., 2023; Ye et al., 2023b,a; Chen\net al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023). BLIP-2 (Li et al., 2023c) proposes Q-Former\nto align the frozen vision foundation models and LLMs. Meanwhile, LLAVA (Liu et al., 2023) and Mini-\nGPT4 (Zhu et al., 2023) introduce visual instruction tuning to enhance instruction following capabilities in\nLVLMs. Additionally, mPLUG-DocOwl (Ye et al., 2023a) incorporates document understanding capabilities\ninto LVLMs by introducing digital documents data. Kosmos2 (Peng et al., 2023), Shikra (Chen et al., 2023a),\nand BuboGPT (Zhao et al., 2023) further enhance LVLMs with visual grounding abilities, enabling region\ndescription and localization. In this work, we integrate image captioning, visual question answering, OCR,\ndocument understanding, and visual grounding capabilities into Qwen-VL. The resulting model achieves\noutstanding performance on these diverse style tasks.\n6 Conclusion and Future Work\nWe release the Qwen-VL series, a set of large-scale multilingual vision-language models that aims to facili-\ntate multimodal research. Qwen-VL outperforms similar models across various benchmarks, supporting\nmultilingual conversations, multi-image interleaved conversations, grounding in Chinese, and fine-grained\nrecognition. Moving forward, we are dedicated to further enhancing Qwen-VL\u2019s capabilities in several key\ndimensions:\n\u2022 Integrating Qwen-VL with more modalities, such as speech and video.\n\u2022 Augmenting Qwen-VL by scaling up the model size, training data and higher resolution, enabling it to\nhandle more complex and intricate relationships within multimodal data.\n\u2022 Expanding Qwen-VL\u2019s prowess in multi-modal generation, specifically in generating high-fidelity\nimages and fluent speech.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. InICCV, 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot\nlearning. InNeurIPS, 2022.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.arXiv:2305.10403, 2023.\nJinzeBai, RuiMen, HaoYang, XuanchengRen, KaiDang, YichangZhang, XiaohuanZhou, PengWang, Sinan\nTan, An Yang, et al. Ofasys: A multi-modal multi-task learning system for building generalist models.\narXiv:2212.04408, 2022.\nShuaiBai,ShushengYang,JinzeBai,PengWang,XingxuanZhang,JunyangLin,XinggangWang,ChangZhou,\nand Jingren Zhou.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49159fff-e568-4517-875b-4cf66e4514be": {"__data__": {"id_": "49159fff-e568-4517-875b-4cf66e4514be", "embedding": null, "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "b6a89eea2f3e1d12356eca35e084d1be79b62a7d1602c687c71af8d2543c404d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e63844d-52d9-4c99-aca8-67f3668f07b7", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "60b7793612e4ee3e3ce1e6dd656cb6ecb6e226723767519b3da935f7539bf96f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.arXiv:2305.10403, 2023.\nJinzeBai, RuiMen, HaoYang, XuanchengRen, KaiDang, YichangZhang, XiaohuanZhou, PengWang, Sinan\nTan, An Yang, et al. Ofasys: A multi-modal multi-task learning system for building generalist models.\narXiv:2212.04408, 2022.\nShuaiBai,ShushengYang,JinzeBai,PengWang,XingxuanZhang,JunyangLin,XinggangWang,ChangZhou,\nand Jingren Zhou. Touchstone: Evaluating vision-language models by language models.arXiv:2308.16890,\n2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nInNeurIPS, 2020.\n10", "mimetype": "text/plain", "start_char_idx": 2709, "end_char_idx": 3549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f361fb5b-4786-409a-aa47-228d4fcd6a0a": {"__data__": {"id_": "f361fb5b-4786-409a-aa47-228d4fcd6a0a", "embedding": null, "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "286111ca07442f3b9471c3907e46c6fae4492ec0e1a8ed8969e31027f30f1bd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a091eb7-4ee9-42e4-a95a-6457e71181eb", "node_type": "1", "metadata": {}, "hash": "ade122511404a656e78e9294de7c624065e5df7216d0ed2ab2fe047795fb2c65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MinwooByeon,BeomheePark,HaecheonKim,SungjunLee,WoonhyukBaek,andSaehoonKim. Coyo-700m:\nImage-text pair dataset, 2022. URLhttps://github.com/kakaobrain/coyo-dataset.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. InCVPR, 2021.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic.arXiv:2306.15195, 2023a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\narXiv:2209.06794, 2022.\nXiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelmeRuiz,\nSebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language\nmodel. arXiv preprint arXiv:2305.18565, 2023b.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and evaluation server.arXiv:1504.00325, 2015.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Universal image-text representation learning. InECCV, 2020.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning.arXiv:2305.06500, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. InICLR, 2021.\nZi-Yi* Dou, Aishwarya* Kamath, Zhe* Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu,\nYann LeCun, Nanyun Peng, Jianfeng Gao, and Lijuan Wang. Coarse-to-fine vision-language pre-training\nwith fusion in the backbone. InNeurIPS, 2022.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui\nYang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language\nmodels. arXiv:2306.13394, 2023.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan\nMarten, MitchellWortsman, DhrubaGhosh, JieyuZhang, etal. Datacomp: Insearchofthenextgeneration\nof multimodal datasets.arXiv:2304.14108, 2023.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a091eb7-4ee9-42e4-a95a-6457e71181eb": {"__data__": {"id_": "1a091eb7-4ee9-42e4-a95a-6457e71181eb", "embedding": null, "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_10", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "286111ca07442f3b9471c3907e46c6fae4492ec0e1a8ed8969e31027f30f1bd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f361fb5b-4786-409a-aa47-228d4fcd6a0a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "820aade2e1a862f45a75dfe955d0bcdda51132b09831dd89813f1c5229a1e3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mme: A comprehensive evaluation benchmark for multimodal large language\nmodels. arXiv:2306.13394, 2023.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan\nMarten, MitchellWortsman, DhrubaGhosh, JieyuZhang, etal. Datacomp: Insearchofthenextgeneration\nof multimodal datasets.arXiv:2304.14108, 2023.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.arXiv:2304.15010, 2023.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. InCVPR, 2023.\nGoogle. Puppeteer, 2023. URLhttps://github.com/puppeteer/puppeteer.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. InCVPR, 2017.\nTanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and Derek Hoiem. Grit: General robust image task\nbenchmark. arXiv:2204.13653, 2022.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. InCVPR, 2018.\n11", "mimetype": "text/plain", "start_char_idx": 2274, "end_char_idx": 3598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40dc2b7e-dec6-4dd0-b737-6e95b5ae49cf": {"__data__": {"id_": "40dc2b7e-dec6-4dd0-b737-6e95b5ae49cf", "embedding": null, "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2945e84823e070905a789a16efbdec806e912ba28e5bc8ee48bb99b105fb11e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7988e148-f9f8-46cf-acee-9d6e49b397f2", "node_type": "1", "metadata": {}, "hash": "006496fbdba12630b882040e627111a73ceece3bf90296f7d2efd2b2c174e471", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a unified transformer. In\nICCV, 2021.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models.arXiv:2302.14045, 2023.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. InCVPR, 2019.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. Openclip, 2021. URLhttps://doi.org/10.5281/zenodo.5143773.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li,\nandTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytextsupervision.\narXiv:2102.05918, 2021.\nKushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations\nvia question answering. InCVPR, 2018.\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in\nphotographs of natural scenes. InEMNLP, 2014.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A\ndiagram is worth a dozen images. InECCV, 2016.\nGeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,WonseokHwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In\nECCV, 2022.\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or\nregion supervision. InICML, 2021.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\nKalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using\ncrowdsourced dense image annotations. InIJCV, 2017.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning.arXiv:2305.03726, 2023a.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension.arXiv:2307.16125, 2023b.\nJunnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi.\nAlign before fuse: Vision and language representation learning with momentum distillation. InNeurIPS,\n2021a.\nJunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. Blip: Bootstrappinglanguage-imagepre-training\nfor unified vision-language understanding and generation. InICML, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models.arXiv:2301.12597, 2023c.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7988e148-f9f8-46cf-acee-9d6e49b397f2": {"__data__": {"id_": "7988e148-f9f8-46cf-acee-9d6e49b397f2", "embedding": null, "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_11", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2945e84823e070905a789a16efbdec806e912ba28e5bc8ee48bb99b105fb11e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40dc2b7e-dec6-4dd0-b737-6e95b5ae49cf", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "599ebef689cc7eb392236d02e971efc4e26f7a2a9ccd34548ebd790d6c37e8f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi.\nAlign before fuse: Vision and language representation learning with momentum distillation. InNeurIPS,\n2021a.\nJunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. Blip: Bootstrappinglanguage-imagepre-training\nfor unified vision-language understanding and generation. InICML, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models.arXiv:2301.12597, 2023c.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. UNIMO:\ntowards unified-modal understanding and generation via cross-modal contrastive learning. InACL, 2021b.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. InECCV, 2020.\n12", "mimetype": "text/plain", "start_char_idx": 2338, "end_char_idx": 3338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "129b4a8b-6e05-4bfc-87a9-65c378b14d89": {"__data__": {"id_": "129b4a8b-6e05-4bfc-87a9-65c378b14d89", "embedding": null, "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2dbfae808b0e7e7c78f2c037ddbc1b2f20d321d3a861b05d8a4037a51be7173e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78d8d705-5b7c-4e05-86b3-82a7d9276620", "node_type": "1", "metadata": {}, "hash": "bdecaeaa1f7efcb7e70138d5cb44842198b07d4af7477fab5c185542ceae3e3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,\nXianyan Jia, et al. M6: A chinese multimodal pretrainer. InKDD, 2021.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. InECCV, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.arXiv:2304.08485,\n2023.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks.arXiv:2206.08916, 2022a.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsforsciencequestion\nanswering. InNeurIPS, 2022b.\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Gener-\nation and comprehension of unambiguous object descriptions. InCVPR, 2016.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. InCVPR, 2019.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for\nquestion answering about charts with visual and logical reasoning.arXiv:2203.10244, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images.\nInWACV, 2021.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question\nanswering by reading text in images. InICDAR, 2019.\nOpenai. Chatml documents. URLhttps://github.com/openai/openai-python/blob/main/chatml.md.\nOpenAI. Gpt-4 technical report, 2023.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned\nphotographs. InNeurIPS, 2011.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world.arXiv:2306.14824, 2023.\nQwen. Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts), 2023.\nURLhttps://github.com/QwenLM/Qwen-7B.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural\nlanguage supervision. InICML, 2021.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.arXiv:2210.08402, 2022a.\nChristoph Schuhmann, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78d8d705-5b7c-4e05-86b3-82a7d9276620": {"__data__": {"id_": "78d8d705-5b7c-4e05-86b3-82a7d9276620", "embedding": null, "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_12", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "2dbfae808b0e7e7c78f2c037ddbc1b2f20d321d3a861b05d8a4037a51be7173e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "129b4a8b-6e05-4bfc-87a9-65c378b14d89", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "4859c89bbb41bcb396c9355c027c31977b6ebe44c2c0004aeab1b83301e37b70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural\nlanguage supervision. InICML, 2021.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.arXiv:2210.08402, 2022a.\nChristoph Schuhmann, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco:\n600m synthetic captions from laion2b-en.https://laion.ai/blog/laion-coco/, 2022b.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-\nnymed, image alt-text dataset for automatic image captioning. InACL, 2018.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioning with reading comprehension. InECCV, 2020.\n13", "mimetype": "text/plain", "start_char_idx": 2232, "end_char_idx": 3255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89a5e170-b073-44e4-a5e7-fe8540b6854e": {"__data__": {"id_": "89a5e170-b073-44e4-a5e7-fe8540b6854e", "embedding": null, "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "f1e58467c3943ea5e79017e74021d0843913de48d68d5438efe23f87e18f1446", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "173d89ec-2055-4dc1-915b-bf2140f37d0e", "node_type": "1", "metadata": {}, "hash": "0951375b46b5433e92ece0d205b30974323e34ef400d2560156b7e9e37060cc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. InCVPR, 2022.\nArtifex Software. Pymupdf, 2015. URLhttps://github.com/pymupdf/PyMuPDF.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic\nvisual-linguistic representations. InICLR, 2019.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,\nTiejun Huang, and Xinlong Wang. Generative pretraining in multimodality.arXiv:2307.05222, 2023.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. InCVPR, 2015.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. InICML, 2022a.\nPeng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and\nChang Zhou. One-peace: Exploring one general representation model toward unlimited modalities.\narXiv:2305.11172, 2023.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan\nMohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all\nvision and vision-language tasks.arXiv:2208.10442, 2022b.\nAn Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip:\nContrastive vision-language pretraining in chinese.arXiv:2211.01335, 2022a.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of gpt-3 for few-shot knowledge-based vqa. InAAAI, 2022b.\nJiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang\nLi, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document\nunderstanding. arXiv:2307.02499, 2023a.\nQinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,Pengcheng\nShi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality.\narXiv:2304.14178, 2023b.\nPeterYoung,AliceLai,MicahHodosh,andJuliaHockenmaier. Fromimagedescriptionstovisualdenotations:\nNew similarity metrics for semantic inference over event descriptions. InACL, 2014.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models.arXiv:2205.01917, 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "173d89ec-2055-4dc1-915b-bf2140f37d0e": {"__data__": {"id_": "173d89ec-2055-4dc1-915b-bf2140f37d0e", "embedding": null, "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_13", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "f1e58467c3943ea5e79017e74021d0843913de48d68d5438efe23f87e18f1446", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89a5e170-b073-44e4-a5e7-fe8540b6854e", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "8c5025a85dd6aa66ae1034022cd331282cdb522074e3932ec8aab6d2950c4fc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mplug-owl: Modularization empowers large language models with multimodality.\narXiv:2304.14178, 2023b.\nPeterYoung,AliceLai,MicahHodosh,andJuliaHockenmaier. Fromimagedescriptionstovisualdenotations:\nNew similarity metrics for semantic inference over event descriptions. InACL, 2014.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models.arXiv:2205.01917, 2022.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel C. F. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,\nXuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan\nWang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan\nZhang. Florence: A new foundation model for computer vision.arXiv:2111.11432, 2021.\nYan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with\nvisual concepts.arXiv:2111.08276, 2021.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas\nBeyer. Lit: Zero-shot transfer with locked-image text tuning. InCVPR, 2022.\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for\nvideo understanding.arXiv:2306.02858, 2023.\n14", "mimetype": "text/plain", "start_char_idx": 2157, "end_char_idx": 3462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6907c28-d1b9-48c3-ae70-db73b1f646c9": {"__data__": {"id_": "c6907c28-d1b9-48c3-ae70-db73b1f646c9", "embedding": null, "metadata": {"page_label": "15", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_14", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "dd50bd53902b030714f6246e95f58fed498fee8788a82e2caf5abc0a521f0572", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng\nGao. Vinvl: Revisiting visual representations in vision-language models. InCVPR, 2021.\nYang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual\ngrounding in multi-modal llms.arXiv:2307.08581, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models.arXiv:2304.10592, 2023.\nXizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver:\nPre-training unified architecture for generic perception for zero-shot and few-shot tasks. InCVPR, 2022.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b91da8b-685b-499c-af34-142575b82eda": {"__data__": {"id_": "4b91da8b-685b-499c-af34-142575b82eda", "embedding": null, "metadata": {"page_label": "16", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_15", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "e884ae0c71c54309852e5464ada87fce9d38d619e4a46582af273eb06cc33464", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Dataset details\nA.1 Image-text pairs\nWe use web-crawled image-text pairs dataset for pre-training, which includes LAION-en (Schuhmann et al.,\n2022a), LAION-zh (Schuhmann et al., 2022a), LAION-COCO (Schuhmann et al., 2022b), DataComp (Gadre\net al., 2023) and Coyo (Byeon et al., 2022). We clean these noisy data by several steps:\n1. Removing pairs with too large aspect ratio of the image\n2. Removing pairs with too small image\n3. Removing pairs with a harsh CLIP score (dataset-specific)\n4. Removing pairs with text containing non-English or non-Chinese characters\n5. Removing pairs with text containing emoji characters\n6. Removing pairs with text length too short or too long\n7. Cleaning the text\u2019s HTML-tagged part\n8. Cleaning the text with certain unregular patterns\nFor academic caption datasets, we remove pairs whose text contains the special tags in CC12M (Changpinyo\net al., 2021) and SBU (Ordonez et al., 2011). If there is more than one text matching the same image, we\nselect the longest one.\nA.2 VQA\nFortheVQAv2(Goyaletal.,2017)dataset,weselecttheanswerannotationbasedonthemaximumconfidence.\nFor other VQA datasets, we didn\u2019t do anything special.\nA.3 Grounding\nFor the GRIT (Peng et al., 2023) dataset, we found that there are many recursive grounding box labels in one\ncaption. We use the greedy algorithm to clean the caption to make sure each image contains the most box\nlabels with no recursive box labels. For other grounding datasets, we simply concatenate the noun/phrase\nwith respective bounding box coordinates.\nA.4 OCR\nWegeneratedthesyntheticOCRdatasetusingSynthdog(Kimetal.,2022). Specifically,weusetheCOCO(Lin\net al., 2014) train2017 and unlabeld2017 dataset split as the natural scenery background. Then we selected 41\nEnglish fonts and 11 Chinese fonts to generate text. We use the default hyperparameters as in Synthdog. We\ntrack the generated text locations in the image and convert them to quadrilateral coordinates and we also use\nthese coordinates as training labels. The visualization example is illustrated in the second row of Fig 5.\nForallthePDFdatawecollected,wefollowthestepsbelowtopre-processthedatausingPyMuPDF(Software,\n2015) to get the rendering results of each page in a PDF file as well as all the text annotations with their\nbounding boxes.\n1. Extracting all texts and their bounding boxes for each page.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bf6477e-b700-4502-afe0-8194e976e8e0": {"__data__": {"id_": "8bf6477e-b700-4502-afe0-8194e976e8e0", "embedding": null, "metadata": {"page_label": "17", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_16", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "b4563731bb9b39dd13914c3a236d4e4a8bcc144d16654d76af666b0e93712481", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 82, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da8ac2e6-58e0-4fd2-bb50-62fd3912e01d": {"__data__": {"id_": "da8ac2e6-58e0-4fd2-bb50-62fd3912e01d", "embedding": null, "metadata": {"page_label": "18", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_17", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "76a99ca59c9b3cdc00bc4d20e72e775642fdbd2dec552444b4d3794263ea2466", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Rendering each page and save them as an image file.\n3. Removing too small image.\n4. Removing images with too many or too few characters.\n5. Removing images containing Unicode characters in the \u201cLatin Extended-A\u201d and \u201cLatin Extended-B\u201d\nblocks.\n6. Removing images containing Unicode characters in the \u201cPrivate Use Area (PUA)\u201d block.\nFor all HTML web pages we collected, we pre-process them in a similar approach to all the PDF data we\ncollected, but we use Puppeteer (Google, 2023) instead of PyMuPDF to render these HTML pages and get\nthe ground truth annotation. We follow the steps below to pre-process the data.\n1. Extracting all texts for each webpage.\n2. Rendering each page and save them as an image file.\n3. Removing too small image.\n4. Removing images with too many or too few characters.\n5. Removing images containing Unicode characters in the \u201cPrivate Use Area (PUA)\u201d block.\nB Data Format Details of Training\nB.1 Data Format of Multi-Task Pre-training\nWe visualize the Multi-Task Pre-training data format in Box B.1. The Box contains all 7 tasks with the\nblack-colored text as the prefix sequence without loss and blue-colored text as the ground truth labels with\nloss.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bbf3636-0872-453e-9a7c-f7b378bcfa16": {"__data__": {"id_": "2bbf3636-0872-453e-9a7c-f7b378bcfa16", "embedding": null, "metadata": {"page_label": "19", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_18", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "f17ac2a7381433e821bdb821dbeb51634bde43e1b2cd14fbf13ee8ad9331d499", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Image Captioning\n<img>cc3m/01581435.jpg</img>Generate the caption in English: the beautiful flowers for\ndesign.<eos>\nVision Question Answering\n<img>VG_100K_2/1.jpg</img> Does the bandage have a different color than the wrist band?\nAnswer: No, both the bandage and the wrist band are white.<eos>\nOCR VQA\n<img>ocr_vqa/1.jpg</img> What is the title of this book? Answer: Asi Se Dice!, Volume 2: Work-\nbook And Audio Activities (Glencoe Spanish) (Spanish Edition)<eos>\nCaption with Grounding\n<img>coyo700m/1.jpg</img>Generate the caption in English with grounding: Beautiful shot of\n<ref>bees</ref><box>(661,612),(833,812)</box><box>(120,555),(265,770)</box> gathering\nnectars from<ref>an apricot flower</ref><box>(224,13),(399,313) </box><eos>\nReferring Grounding\n<img>VG_100K_2/3.jpg</img><ref>the ear on a giraffe </ref><box>(176,106),(232,160)\n</box><eos>\nGrounded Captioning\n<img>VG_100K_2/4.jpg</img><ref>This</ref><box>(360,542),(476,705)</box> isYellowcross\ncountry ski racing gloves<eos>\nOCR\n<img>synthdog/1.jpg</img>OCR with grounding:<ref>It is managed</ref> <quad> (568,121),\n(625,131), (624,182), (567,172)</quad>...<eos>\nB.2 Data Format of Supervised Fine-tuning\nTobetteraccommodatemulti-imagedialogueandmultipleimageinputs,weaddthestring\"Picture id:\"before\ndifferent images, where theid corresponds to the order of image input dialogue. In terms of dialogue format,\nwe construct our instruction tuning dataset using the ChatML (Openai) format, where each interaction\u2019s\nstatement is marked with two special tokens (<im_start> and <im_end>) to facilitate dialogue termination.\nThe Dataset Format Example of ChatML\n<im_start>user\nPicture 1:<img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>\n<im_start>assistant\nThe sign is a road closure with an orange rhombus.<im_end>\n<im_start>user\nHow is the weather in the picture?<im_end>\n<im_start>assistant\nThe shape of the road closure sign is an orange rhombus.<im_end>\nDuringtraining,weensuretheconsistencybetweenpredictionandtrainingdistributionsbyonlysupervising\nanswers and special tokens (blue in the example), and not supervising role names or question prompts.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ed1ff13-4a78-4a32-9a82-19f84b6e97de": {"__data__": {"id_": "1ed1ff13-4a78-4a32-9a82-19f84b6e97de", "embedding": null, "metadata": {"page_label": "20", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_19", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "776607cddbab0cea3594f2ac5bfa630eb27e1376c1e09b2983c5b1ec546f17c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C Hyperparameters\nWe report the detailed training hyperparameter settings of Qwen-VL in Table 8.\nTable 8: Training hyperparameters of Qwen-VL\nConfiguration Pre-training Multi-task Pre-training Supervised Fine-tuning\nViT init. Open-CLIP-bigG Qwen-VL 1st-stage Qwen-VL 2nd-stage\nLLM init. Qwen-7B Qwen-7B Qwen-VL 2nd-stage\nVL Adapter init. random Qwen-VL 1st-stage Qwen-VL 2nd-stage\nImage resolution 2242 4482 4482\nViT sequence length 256 1024 1024\nLLM sequence length 512 2048 2048\nLearnable query numbers 256 256 256\nOptimizer AdamW\nOptimizer hyperparameter \u03b21 = 0.9, \u03b22 = 0.98, eps= 1e\u22126\nPeak learning rate 2e\u22124 5e\u22125 1e\u22125\nMinimum learning rate 1e\u22126 1e\u22125 1e\u22126\nViT learning rate decay 0.95 0.95 0\nViT Drop path rate 0\nLearning rate schedule cosine decay\nWeight decay 0.05\nGradient clip 1.0\nTraining steps 50k 19k 8k\nWarm-up steps 500 400 3k\nGlobal batch size 30720 4096 128\nGradient Acc. 6 8 8\nNumerical precision bfloat16\nOptimizer sharding \u2713\nActivation checkpointing \u2717\nModel parallelism \u2717 2 2\nPipeline parallelism \u2717\nIn the first pre-training stage, the model is trained using AdamW optimizer with\u03b21 = 0.9, \u03b22 = 0.98, eps=\n1e\u22126. We use the cosine learning rate schedule and set the maximum learning rate of2e\u22124 and minimum of\n1e\u22126 with a linear warm-up of 500 steps. We use a weight decay of5e\u22122 and a gradient clipping of1.0. For\nthe ViT image encoder, we apply a layer-wise learning rate decay strategy with a decay factor of0.95. The\ntraining process uses a batch size of 30720 for the image-text pairs, and the entire first stage of pre-training\nlasts for 50,000 steps, consuming approximately 1.5 billion image-text samples and 500 billion image-text\ntokens.\nIn the second multi-task training stage, we increase the input resolution of the visual encoder from224 \u00d7 224\nto448 \u00d7448,reducingtheinformationlosscausedbyimagedown-sampling. Weunlockedthelargelanguage\nmodel and trained the whole model. The training objective is the same as the pre-training stage. We use\nAdamW optimizer with\u03b21 = 0.9, \u03b22 = 0.98, eps= 1e\u22126. We trained for 19000 steps with 400 warm-up steps\nand a cosine learning rate schedule. Specifically, we use the model parallelism techniques for ViT and LLM.\nD Summary of the evaluation benchmarks\nWe provide a detailed summary of the used evaluation benchmarks and corresponding metrics in Table 9.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4ea8c22-1a9f-46dd-ae09-30d0c94fb6a4": {"__data__": {"id_": "f4ea8c22-1a9f-46dd-ae09-30d0c94fb6a4", "embedding": null, "metadata": {"page_label": "21", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_20", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "5006d89cd7bfce8019f578ee813bfa3dc95fa0b187cb7bc730ca45c0abfd493d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 9: Summary of the evaluation benchmarks.\nTask Dataset Description Split Metric\nImage Caption Nocaps Captioning of natural images val CIDEr(\u2191)\nFlickr30K Captioning of natural images karpathy-test CIDEr(\u2191)\nGeneral VQA\nVQAv2 VQA on natural images test-dev VQA Score(\u2191)\nOKVQA VQA on natural images requiring outside knowledgeval VQA Score(\u2191)\nGQA VQA on scene understanding and reasoning test-balanced EM(\u2191)\nScienceQA-Img Multi-choice VQA on a diverse set of science topicstest Accuracy(\u2191)\nVizWiz VQA on photos taken by people who are blind test-dev VQA Score(\u2191)\nText-oriented VQA\nTextVQA VQA on natural images containing text val VQA Score(\u2191)\nDocVQA VQA on images of scanned documents test ANLS(\u2191)\nChartQA VQA on images of charts test Relaxed EM(\u2191)\nOCRVQA VQA on images of book covers test EM(\u2191)\nAI2Diagram VQA on images of scientific diagrams test EM(\u2191)\nRefCOCO Refer grounding on natural images val & testA & testB Accuracy(\u2191)\nRefer Expression RefCOCO+ Refer grounding on natural images val & testA & testB Accuracy(\u2191)\nComprehension RefCOCOg Refer grounding on natural images val & test Accuracy(\u2191)\nGRiT Refer grounding on natural images test Accuracy(\u2191)\nInstruction Following\nTouchStone Open-ended VL instruction following benchmark English & Chinese GPT-4 Score (\u2191)\nMME Open-ended VL Benchmark by yes/no questions Perception & Cognition Accuracy (\u2191)\nSeed-Bench Open-ended VL Benchmark by Multi-choice VQA Image & Video Accuracy (\u2191)\nE Additional experimental details\nE.1 Convergence of the Pre-training Stage\nIn Figure 6, we show the convergence of the Pre-training Stage (stage one). The whole models are trained\nusing BFloat16 mixed precision, the batch size is 30720, and the learning rate is2e\u22124. All images are only\ntrained once (one epoch). The training loss decreases steadily with the increase of the number of training\npictures. Note that, the pre-training stage (Stage one) has no VQA data being added, but the Zero-shot VQA\nscore increases amidst fluctuations.\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\n#Images(B)\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0Loss\na. Pre-training Loss\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\n#Images(B)\n62\n64\n66\n68\n70\n72\n74\n76CIDEr\nb. Caption (Flickr)\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\n#Images(B)\n48\n50\n52\n54\n56Accuracy\nc. Zero-shot VQA (VQAv2)\nFigure 6: Visualization of the Convergence of the Pre-training Stage\nE.2 Number of Learnable Queries in the Vision-Language Adapter\nThe vision-language adapter uses cross-attention to compress the visual feature sequence by a set of learning\nqueries of length. Too few queries can lead to the loss of some visual information, while too many queries\nmay reduce in greater convergence difficulty and computational cost.\nAn ablation experiment is conducted on the number of learnable queries in the vision-language adapter. We\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1ef78bf-7dad-4493-b3c0-c7ce9591f813": {"__data__": {"id_": "d1ef78bf-7dad-4493-b3c0-c7ce9591f813", "embedding": null, "metadata": {"page_label": "22", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_21", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "94c1a44a5f9cb77935bc774544c415a028f5cc21697d96971392bb6e05c56152", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 20 30 40 50\nSteps\n3\n4\n5\n6\n7\n8\n9\n10\n11Loss\nL64\nL144\nL256\nL400\n1000 1500 2000 2500 3000 3500 4000 4500\nSteps\n2.40\n2.45\n2.50\n2.55\n2.60\n2.65\n2.70Loss\nL64\nL144\nL256\nL400\nFigure 7: Visualization of the training loss when using different compressed feature lengths of the vision-\nlanguage adapter. The left depicts the initial training loss (within 50 steps), and the right depicts the loss in\nconvergence (1k-5k steps). In the legend, L64 denotes that the adapter uses 64 queries to compress the visual\nfeature sequence to a fixed length of 64, and so on. The loss curves have been smoothed to avoid shading\nowing to fluctuations.\nused ViT-L/14 as the visual encoder and the224 \u00d7 224 resolution picture as input, so the sequence length\nof ViT\u2019s output is(224/14)2 = 256. As shown in the left part of Figure 7, the fewer queries used at the\nbeginning of training, the lower the initial loss. However, with convergence, too many or too few queries\nwill cause convergence to slow down, as shown in the right part of Figure 7. Considering that the second\ntraining stage (Multi-task Pre-train) applies 448*448 resolution, where the sequence length of ViT\u2019s output\nis(448/14)2 = 1024. Too few queries can result in more information being lost. We finally chose to use 256\nqueries for the vision-language adapter in Qwen-VL.\nE.3 Window Attention vs Global Attention for Vision Transformer\nUsing a high-resolution Vision Transformer in the model will significantly increase the computational cost.\nOnepossiblesolutiontoreducethecomputationalcostofthemodelistouseWindowAttentionintheVision\nTransformer, i.e., to perform Attention only in a window of224 \u00d7 224 in most layers of the ViT part of the\nmodel, and to perform Attention for the full448 \u00d7 448 or 896 \u00d7 896 image in a small number of layers (e.g. 1\nout of every 4 layers) of the ViT part of the model.\nTothisend,weconductedablationexperimentstocomparetheperformanceofthemodelwhenusingGlobal\nAttention and Window Attention for ViT. We compare the experimental results for analysing the trade-off\nbetween computational efficiency and convergence of the model.\nTable 10: Training speed of Window Attention vs Global Attention for different input image resolutions\nModel input resolution & Attention type Training speed\n448 \u00d7 448, Global Attention 10s / iter\n448 \u00d7 448, Window Attention 9s / iter\n896 \u00d7 896, Global Attention 60s / iter\n896 \u00d7 896, Window Attention 25s / iter\nAs shown in Figure 8 and Table 10, the loss of the model is significantly higher when Window Attention\ninstead of Vanilla Attention is used. And the training speeds for both of them are similar. Therefore, we\ndecided to use Vanilla Attention instead of Window Attention for the Vision Transformer when training\nQwen-VL.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "704d6582-6784-424c-9bf7-135ca5ec149f": {"__data__": {"id_": "704d6582-6784-424c-9bf7-135ca5ec149f", "embedding": null, "metadata": {"page_label": "23", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_22", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "4a62fe12b0680f1cef8e266cce1f2d7fdf9c22e2ce2bb6167a507c61da1c5b05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8: Visualization of the Loss when using Window Attention vs Global Attention\nThe reason we don\u2019t use Window Attention with896 \u00d7 896 resolution is that its training speed is too slow for\nus. Although it reaches a loss value similar to model with448 \u00d7 448 resolution input at 5000 steps. It takes\nalmost 2.5 times longer to train than the model with448 \u00d7 448 resolution input.\nE.4 Performance on Pure-text Tasks\nInordertostudytheeffectofmulti-modaltrainingonpure-textability,weshowtheperformanceofpure-text\ntasks of Qwen-VL compared to open-source LLM in Table 11.\nQwen-VL uses an intermediate checkpoint of Qwen-7B as the LLM initialization. The reason why we did\nnot use the final released checkpoint of Qwen-7B is that Qwen-VL and Qwen-7B were developed at a very\nsimilar period. Because Qwen-VL has a good initialization on LLM by Qwen-7B, it is comparable to many\ntext-only LLMs on pure-text tasks.\nTable 11: Performance on Pure-text Benchmarks of Qwen-VL compared to open-source LLM. Due to the\nintroduction of pure-text data in the multi-task training and SFT stage, Qwen-VL do not compromise any\npure-text ability.\nModel MMLU CMMLU C-Eval\nLLaMA-7B 35.1 26.8 -\nLLaMA2-7B 46.8 31.8 32.5\nBaichuan-7B 42.3 44.4 42.8\nBaichuan2-7B 54.2 57.1 54.0\nChatGLM2-6B 47.9 48.8 51.7\nInternLM-7B 51.0 51.8 52.8\nQwen-7B (final released) 58.2 62.2 63.5\nQwen-7B (intermediate, use as Qwen-VL\u2019s LLM initialization) 49.9 - 48.5\nQwen-VL 50.7 49.5 51.1\nFurthermore,inthemulti-tasktrainingandSFTstages,Qwen-VLnotonlyutilizesvisualandlanguage-related\ndata but also incorporates pure-text data for training. The purpose of this is to prevent the catastrophic\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73f1b6a3-cd38-4aac-bbf4-9cd14d6f437c": {"__data__": {"id_": "73f1b6a3-cd38-4aac-bbf4-9cd14d6f437c", "embedding": null, "metadata": {"page_label": "24", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf_part_23", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_path": "/home/chen/cyh/LLM/project1/Langchain_llamaindex_agent/kb/zhipuAI/files/Bai \u7b49 - 2023 - Qwen-VL A Versatile Vision-Language Model for Und.pdf", "file_type": "application/pdf", "file_size": 6329953, "creation_date": "2025-12-16", "last_modified_date": "2025-12-16"}, "hash": "b6497261e5a69eae33374beb98dda6bd668827fa0e1bd668ebb5b07cb5100950", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "forgetting of text comprehension by leveraging the information from pure-text data. The results in Table 11\nindicate that the Qwen-VL model does not exhibit any degradation in terms of its pure text capability and\neven demonstrates improvement after multi-task training.\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}