import streamlit as st
import uuid
import os
from pathlib import Path

from utils import PLATFORMS, get_llm_models, get_chatllm, get_kb_names, get_img_base64
from langchain_core.messages import AIMessageChunk, ToolMessage
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.prebuilt import ToolNode, tools_condition
from tools.llamaindex_tool import get_llamaindex_tool

RAG_PAGE_INTRODUCTION = "ä½ å¥½ï¼Œæˆ‘æ˜¯æ™ºèƒ½åŠ©æ‰‹ï¼Œå½“å‰é¡µé¢ä¸º`RAG å¯¹è¯æ¨¡å¼`ï¼Œå¯ä»¥åœ¨å¯¹è¯è®©å¤§æ¨¡å‹åŸºäºå·¦ä¾§æ‰€é€‰çŸ¥è¯†åº“è¿›è¡Œå›ç­”ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"

# ä½¿ç”¨ st.cache_resource é¿å…æ¯æ¬¡åˆ·æ–°é¡µé¢éƒ½é‡å»ºç´¢å¼•
@st.cache_resource(show_spinner=False)
def load_kb_tool(kb_name, kb_path):
    return get_llamaindex_tool(kb_name, kb_path)

# Graph Response å¤„ç†é€»è¾‘
def graph_response(graph, input):
    for event in graph.stream(
        {"messages": input},
        config={"configurable": {"thread_id": str(uuid.uuid4())}}, 
        stream_mode="messages",
    ):
        if type(event[0]) == AIMessageChunk:
            if len(event[0].tool_calls):
                st.session_state["rag_tool_calls"].append(
                    {
                        "status": "æ­£åœ¨æŸ¥è¯¢...",
                        "knowledge_base": event[0].tool_calls[0]["name"].replace("_knowledge_base_tool", ""),
                        "query": str(event[0].tool_calls[0]["args"].get("query", "æ— æŸ¥è¯¢å‚æ•°")),
                    }
                )
            yield event[0].content
        elif type(event[0]) == ToolMessage:
            if len(st.session_state["rag_tool_calls"]) > 0:
                st.session_state["rag_tool_calls"][-1]["status"] = "å·²å®Œæˆå·¥å…·è°ƒç”¨"
                st.session_state["rag_tool_calls"][-1]["content"] = event[0].content

# è·å–å›ç­”çš„ä¸»é€»è¾‘
def get_rag_chat_response(platform, model, temperature, messages, selected_kbs, KBS, base_url, api_key):
    if not selected_kbs:
        yield "è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©è‡³å°‘ä¸€ä¸ªçŸ¥è¯†åº“ã€‚"
        return

    tools = [KBS[k] for k in selected_kbs if k in KBS]
    
    if not tools:
        yield "é€‰ä¸­çš„çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½ï¼Œè¯·æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€ã€‚"
        return

    tool_node = ToolNode(tools=tools)

    def call_model(state):
        llm = get_chatllm(platform, model, base_url=base_url, api_key=api_key, temperature=temperature)
        llm_with_tools = llm.bind_tools(tools)
        return {"messages": [llm_with_tools.invoke(state["messages"])]}

    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", tool_node)
    workflow.add_conditional_edges("agent", tools_condition)
    workflow.add_edge("tools", "agent")
    workflow.set_entry_point("agent")
    
    graph = workflow.compile()
    yield from graph_response(graph, messages)

# ä¿®å¤ display_chat_history æŠ¥é”™
def display_chat_history():
    for message in st.session_state["rag_chat_history_with_tool_call"]:
        with st.chat_message(message["role"], avatar=get_img_base64("robot.png") if message["role"] == "assistant" else None):
            if "tool_calls" in message.keys():
                for tool_call in message["tool_calls"]:
                    status = tool_call.get("status", "å·²å®Œæˆå·¥å…·è°ƒç”¨")
                    with st.status(status, expanded=False):
                        st.write("å·²è°ƒç”¨çŸ¥è¯†åº“: ", tool_call.get("knowledge_base", "æœªçŸ¥"))
                        if "query" in tool_call:
                            st.write("æŸ¥è¯¢è¯­å¥:")
                            st.code(tool_call.get("query", ""), wrap_lines=True)
                        st.write("çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼š")
                        content = tool_call.get("content")
                        if content:
                            st.write(content)
                        else:
                            st.warning("âš ï¸ å·¥å…·è°ƒç”¨ä¸­æ–­æˆ–æœªè¿”å›ç»“æœ")
            st.write(message.get("content", ""))

# ä¸»é¡µé¢å‡½æ•°
def rag_chat_page():
    kbs = get_kb_names()
    KBS = dict()
    kb_root = Path(__file__).resolve().parents[1] / "kb"
    
    for k in kbs:
        kb_path = kb_root / k
        tool = load_kb_tool(k, kb_path)
        if tool:
            KBS[k] = tool

    if "rag_chat_history" not in st.session_state:
        st.session_state["rag_chat_history"] = [{"role": "assistant", "content": RAG_PAGE_INTRODUCTION}]
    if "rag_chat_history_with_tool_call" not in st.session_state:
        st.session_state["rag_chat_history_with_tool_call"] = [{"role": "assistant", "content": RAG_PAGE_INTRODUCTION}]
    if "rag_tool_calls" not in st.session_state:
        st.session_state["rag_tool_calls"] = []

    # --- ä¾§è¾¹æ é…ç½® (å°†é…ç½®å’Œæ¸…ç©ºç§»åˆ°è¿™é‡Œ) ---
    with st.sidebar:
        st.subheader("ğŸ¤– æ¨¡å‹é…ç½®")
        platform = st.selectbox("æ¨¡å‹å¹³å°", PLATFORMS)
        llm_models = get_llm_models(platform, st.session_state.get("base_url", ""), st.session_state.get("api_key", ""))
        if not llm_models: llm_models = ["åŠ è½½å¤±è´¥æˆ–åˆ—è¡¨ä¸ºç©º"]
        model = st.selectbox("é€‰æ‹©æ¨¡å‹", llm_models)
        temperature = st.slider("Temperature", 0.1, 1., 0.1)
        history_len = st.slider("å†å²æ¶ˆæ¯é•¿åº¦", 1, 10, 5)
        
        st.divider()
        st.subheader("ğŸ“š çŸ¥è¯†åº“é€‰æ‹©")
        selected_kbs = st.sidebar.multiselect("é€‰æ‹©çŸ¥è¯†åº“", kbs)
        
        st.divider()
        st.subheader("ğŸ”‘ å¯†é’¥é…ç½®")
        base_url = st.text_input("Base URL", help="å¦‚ Ollama æˆ– DashScope çš„ base_url", key="base_url")
        api_key = st.text_input("API Key", help="API Key", type="password", key="api_key")
        
        st.divider()
        # æ¸…ç©ºæŒ‰é’®æ”¾è¿™é‡Œ
        st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", on_click=lambda: st.session_state.update({
            "rag_chat_history": [{"role": "assistant", "content": RAG_PAGE_INTRODUCTION}],
            "rag_chat_history_with_tool_call": [{"role": "assistant", "content": RAG_PAGE_INTRODUCTION}],
            "rag_tool_calls": []
        }), use_container_width=True)

    # ä¸»åŒºåŸŸæ˜¾ç¤ºå†å²
    display_chat_history()

    # --- âœ… ä¿®å¤ç‚¹ï¼šchat_input ç§»å‡º columnsï¼Œç‹¬å ä¸»å±‚çº§ ---
    # è¿™æ ·å®ƒå°±ä¼šè‡ªåŠ¨å¸é™„åœ¨é¡µé¢æœ€åº•éƒ¨
    input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
    # --------------------------------------------------
    
    if input:
        with st.chat_message("user"):
            st.write(input)
        st.session_state["rag_chat_history"] += [{"role": 'user', "content": input}]
        st.session_state["rag_chat_history_with_tool_call"] += [{"role": 'user', "content": input}]

        stream_response = get_rag_chat_response(
            platform, model, temperature,
            st.session_state["rag_chat_history"][-history_len:],
            selected_kbs, KBS,
            st.session_state.get("base_url", ""), st.session_state.get("api_key", "")
        )

        with st.chat_message("assistant", avatar=get_img_base64("robot.png")):
            response = st.write_stream(stream_response)
        
        st.session_state["rag_chat_history"] += [{"role": 'assistant', "content": response}]
        st.session_state["rag_chat_history_with_tool_call"] += [{
            "role": 'assistant', "content": response, "tool_calls": st.session_state["rag_tool_calls"]
        }]
        st.session_state["rag_tool_calls"] = []